1. Introduction to the Temporal Data SpecificationThis document outlines a comprehensive technical specification for a blog feature that enables the recording, representation, and sorting of temporal data. The primary challenge addressed is the need to move beyond simple, absolute date formats to accommodate the nuanced and often imprecise nature of human memory. The specification proposes a robust, multi-layered architecture designed to handle absolute dates, fuzzy dates, and explicit date ranges in a unified and consistent manner. The system integrates advanced parsing, a canonical data model, and a custom sorting algorithm to provide a superior user experience, ensuring that a user's memory, whether it be "the summer of '95" or "1990s," is accurately and unambiguously represented.The architectural approach is founded on three core principles that guide every design decision: Unambiguity, Granularity, and Adaptability. Unambiguity mandates that all internal data representations must be machine-readable and free from the inherent human confusion that arises from common notations like 01/02/03.1 This is achieved by adopting and enforcing a single canonical data format. Granularity requires that the system precisely stores the level of detail provided by the user, whether it is a specific day, a month, a year, a decade, or a broader epoch.3 This ensures that the original intent of the user is preserved, rather than being forced into a more precise-but-false representation. Finally, Adaptability dictates that the system must be flexible enough to handle a wide spectrum of user input, including conversational or relative terms, and that it can evolve as users refine their memories with more specific information over time.2. Data Modeling and Representation: The Core of the SpecificationThe Inadequacy of Standard Data Types for Temporal UncertaintyA fundamental flaw in many systems that handle temporal data is the reliance on standard date and time objects, such as those found in JavaScript or SQL.5 These data types are optimized for representing a single, precise point in time, defined down to the millisecond relative to a fixed epoch.5 This model is fundamentally insufficient for the task at hand. For instance, a fuzzy date like "the 1990s" represents an interval, not a single point. Storing this a a simple date, such as 1990-01-01, a fundamental flaw in many systems that handle temporal data is the reliance on standard date and time objects, such as those found in JavaScript or SQL.5 These data types are optimized for representing a single, precise point in time, defined down to the millisecond relative to a fixed epoch.5 This model is fundamentally insufficient for the task at hand. For instance, a fuzzy date like "the 1990s" represents an interval, not a single point. Storing this a a simple date, such as 1990-01-01, fundamentally corrupts the data. This approach loses the user's original intent—that the event occurred somewhere within a decade—and forces an arbitrary precision that is semantically incorrect.7 This creates significant data anomalies that are difficult to query and sort accurately and can lead to a breakdown of referential integrity.8The objective is to accurately represent the temporal uncertainty of human memory. A user's recollection of "a year and a day" is an interval, not a point in time.10 To model this accurately, the underlying data structure must be capable of representing this interval natively, rather than approximating it with a point. This approach preserves the semantic integrity of the data and provides a robust foundation for all subsequent system operations, including sorting, searching, and visualization.Adopting the Extended Date/Time Format (EDTF) as the Canonical ModelThe solution to the limitations of standard date types is to adopt a more expressive canonical model: the Extended Date/Time Format (EDTF). EDTF is an international standard (ISO 8601-2) that is specifically designed to handle the nuances of uncertain and approximate temporal expressions.4 By implementing EDTF as the core data model, the system can represent a wide range of temporal expressions without loss of information.Key features of EDTF that are leveraged in this specification include:Uncertainty and Approximation: The characters ? and ~ can be appended to a date string to denote uncertainty or approximation. For example, 1984? represents a year that is possibly 1984, but not definitively so, while 2004-06~ signifies an approximate month.4Ranges: EDTF uses a forward slash / to denote a time interval between a start and end date. A decade like "the 1990s" can be represented as the range 1990/1999.4 The standard also supports open-ended ranges, such as [..1984] for "sometime before 1984".11Unspecified Digits: For cases of extreme fuzziness, such as "sometime in the 1560s," EDTF allows the use of the character X to represent an unknown digit. For example, 156X would be parsed as a date within that decade.4Collections: EDTF can also represent a set of discrete, possible dates using square brackets. For example, [1667,1668,1670..1672] represents one of several possible years.4The adoption of EDTF is not merely a technical decision; it is a strategic one. It aligns the system with best practices for Linked Data and semantic interoperability, as espoused by standards like schema.org and JSON-LD.13 This makes the specification a robust and future-proof solution, compatible with a growing ecosystem of systems and AI models designed for temporal reasoning.16Canonical Representation of Date TypesTo ensure consistency and enforce the core principles, all temporal data will be stored internally in a canonical format.Absolute Dates: Represented by a standard ISO 8601 date string, such as 2024-01-15.Fuzzy Dates: Represented as an EDTF string to capture the level of imprecision provided by the user. For example, a user input of "1990s" would be stored as 199X or the more machine-friendly range 1990-01-01/1999-12-31.Date Ranges: Stored using the ISO 8601-2 interval format, e.g., 2024-01-01/2024-03-31 for the first quarter of 2024.4Relative Dates: These are dynamic expressions (e.g., "last month") that are calculated against a NOW timestamp.6 They are treated as a special case during the parsing stage, where they are converted into an EDTF range. The original input is preserved in the database to allow the user to refine the date at a later time. For example, "last month" might be calculated to be the EDTF range 2024-03, which is then stored.The core data structure is defined in the table below. This schema is designed to represent the complex EDTF strings while simultaneously providing simple, standard date objects (sort_start_date, sort_end_date) that can be used for efficient, index-based sorting and timeline visualization.Field NameData TypeDescriptionExampleprimary_idUUIDUnique identifier for the memory/event.81d4512b-b892-4f38-9e66-7b561c28f32ccanonical_dateEDTF StringThe core date representation of the event.1995-07-21? (Uncertain), 199X (Decade), 2004/2006 (Range)sort_start_dateISO 8601 DateThe earliest possible date in the canonical_date range. Used for sorting.1995-07-21 for 1995-07-21?; 1990-01-01 for 199Xsort_end_dateISO 8601 DateThe latest possible date in the canonical_date range. Used for sorting.1995-07-21 for 1995-07-21?; 1999-12-31 for 199XgranularityEnum ('YYYY', 'YYYY-MM', etc.)The precision of the original user input.YYYY, YYYY-MM-DDconfidence_scoreFloat (0-1)A numerical representation of the system's certainty.0.9 (High confidence), 0.6 (Low confidence)original_inputTextThe raw user-provided string for future re-evaluation."I think it was around July 1995"This table serves as the central reference for the entire specification. It shows how the system bridges the gap between a complex, semantically rich data type (canonical_date) and the pragmatic needs of a database (sort_start_date, sort_end_date), creating a solution that is both powerful and performant.Blog-Specific Frontmatter for Temporal ContextThe specification is designed for a blog where a post's publication date and the date of a specific memory or event can be different. To accommodate this, the system will use a standardized frontmatter schema. This schema ensures a clear separation between a blog post's metadata and the temporal data of the event it describes, which is crucial for building a canonical timeline.Blog platforms like Jekyll and Hugo use frontmatter to store metadata such as date and title.19 The proposed schema extends this with a custom field to store the canonical EDTF date. This approach is a best practice, as changing the standard date variable, which is often used for permalinks, can lead to broken links.21The proposed frontmatter properties are:date: This standard property represents the blog post's publication date. It's a precise ISO 8601 string (YYYY-MM-DD HH:MM:SS +/-TTTT) and should be used by the blog engine for its own purposes, such as generating chronological lists of posts.20 This is the "present tense" date for the blog post itself.lastmod (optional): This field can be used to track the date a post was last modified. This is a common feature in blog platforms and is supported by standards like schema.org.13event_date (custom): This is the core property for the memory-based timeline. This custom variable will store the canonical EDTF string that represents the event being described in the post. For a post titled "Remembering the 90s," this field would contain the EDTF range 199X.4 The system's parsing and normalization logic, as described in the previous sections, will populate this field. For posts that are written in the present tense, this field would be omitted.event_date_start (computed): A hidden or system-generated field that stores the earliest possible date from event_date as a standard ISO 8601 date. This field is used as a primary key for sorting the canonical timeline.event_date_end (computed): A hidden or system-generated field that stores the latest possible date from event_date as a standard ISO 8601 date. This field is used as a secondary key for sorting.Here is an example of a blog post's frontmatter using the YAML format:YAML---
title: "The Summer I Worked at the Video Store"
date: 2024-04-15T10:30:00-07:00
lastmod: 2024-04-15T10:30:00-07:00
event_date: 1995-06-XX/1995-08-XX
---
This structure enables a single blog to have two parallel timelines: a standard publication timeline sorted by date, and a "memory timeline" sorted by event_date_start. The system is responsible for converting ambiguous natural language, such as "in the summer of '95," into the precise EDTF format 1995-06-XX/1995-08-XX and populating the computed fields for sorting. This fulfills the requirement for a canonical timeline based on a standardized property.3. User Input, Parsing, and ValidationThe Problem: Ambiguity in Human Temporal ExpressionUser-provided temporal information is rarely clean or formatted for machine consumption. People use natural language, local conventions (02/03/2025 is ambiguous), and relative expressions like yesterday or last year.22 A rigid parser that fails on conversational input or ambiguous formats will create a frustrating user experience. The objective is to design a system that not only parses the date but also understands the user's intent and can gracefully handle imprecision, suggesting a plausible interpretation without forcing a false one.Multi-Layered Parsing ArchitectureThe system employs a two-tiered parsing mechanism to address the spectrum of user input. This tiered approach prioritizes speed and determinism for well-structured inputs while providing a flexible, semantic layer for ambiguous or conversational text.Tier 1: Deterministic Rule-Based Parsing: This initial layer handles all inputs that conform to standard, unambiguous date formats. It quickly processes ISO 8601 strings and other common formats. This layer includes robust validation to ensure calendrical accuracy (e.g., rejecting 2024-02-30).24 This tier is fast, low-cost, and serves as the primary processing engine for a majority of well-formed inputs.Tier 2: LLM-Assisted Semantic Parsing for "Grace": If the rule-based parser fails to interpret the input or if the format is ambiguous (e.g., 02/03/2025), the input is passed to a large language model (LLM). The LLM's role is not to be a primary date validation engine but to act as a semantic layer, interpreting the natural language and returning a structured, canonical output.25 The concept of "grace" is implemented at this stage. Instead of returning a single corrected date, the LLM is prompted to suggest a plausible range and a confidence score. The system then presents this parsed information back to the user for confirmation (e.g., "Is this what you meant: 1985~?"). This interactive feedback loop is a best practice for handling ambiguous user input.22LLM Instructions for Temporal Correction (Grace Period)The LLM-assisted parsing component is a critical part of the system's design. The prompt engineering strategy is a two-step process to ensure reliability and minimize API costs.27Step 1: Transformation to a Structured, Canonical Format: The initial prompt instructs the LLM to act as a semantic parser. It takes the user's query and a set of predefined temporal anchors (TODAY, LAST_WEEK, YESTERDAY) and translates it into a structured JSON object. This JSON contains the canonical EDTF strings and a confidence score. The prompt is designed with few-shot examples to guide the LLM's behavior towards a deterministic, structured output.28 For instance, a query like "I think it was around July 1995" is transformed into a JSON object that includes the EDTF string 1995-07~? and a confidence score. The prompt explicitly specifies that the LLM should not attempt to compute relative dates, but rather to return a formula for the application to resolve.Step 2: Validation and Refinement: The structured JSON from the first step is processed by the application's business logic to resolve any relative dates or formulas. The system then takes the final parsed EDTF string and validates it. The confidence score is used to determine whether the user needs to be prompted for confirmation.For optimal performance and reliability, the LLM should be configured with a low temperature and top_p value. This encourages deterministic, factual output rather than creative or uncertain responses, which is crucial for data cleaning and validation tasks.13User InputRule-Based Output (if applicable)LLM-Assisted Output (Canonical EDTF)LLM Confidence"1990"1990-01-0119901.0"the 1990s"null199X0.9"early 1990"null1990-01~0.7"the 80s"null198X0.9"last month"null``1.0"I think it was around July 1995"null1995-07~?0.6This table provides concrete examples of the normalization process, demonstrating how the system translates natural language into the canonical data model. It highlights the role of both the deterministic parser and the LLM and the importance of capturing the user's intent and level of certainty.4. Timeline Sorting and Visualization LogicThe Sorting Problem for Intervals and PointsThe primary objective of the system's timeline visualization is to present a chronological, ordered list of a user's memories. This task is complicated by the presence of heterogeneous temporal data, including absolute dates, fuzzy dates, and date ranges. Standard sorting algorithms are not designed to handle a mixed collection of points and intervals without an explicit comparison key.30 Simply sorting by a single date field, as some systems do, results in a loss of precision and an inaccurate representation of the data.7 A more sophisticated sorting logic is required to ensure a stable, predictable, and user-friendly timeline.Proposed Sorting Strategy: The Multi-Level Comparison FunctionThe sorting logic is a custom comparison function that operates on the sort_start_date and sort_end_date fields from the canonical data model. This approach is consistent with best practices for sorting and merging temporal intervals.31 The function prioritizes chronological order while using additional data points as tie-breakers. The use of a stable sorting algorithm is recommended to ensure that items with identical keys are not arbitrarily reordered.30 Timsort or Merge Sort are excellent candidates for this due to their stability and high performance on real-world data.33The sorting strategy is as follows:Primary Key: The list is first sorted in ascending order based on the sort_start_date. This is the most intuitive approach for arranging events chronologically and is a pattern validated by existing systems that handle fuzzy dates.7Secondary Key (Tie-breaker 1): For events with an identical sort_start_date, the system sorts by sort_end_date in ascending order. This ensures that shorter intervals appear before longer ones that begin on the same day. For example, a memory from "1990" (1990-01-01/1990-12-31) will be placed before a memory from the "1990s" (1990-01-01/1999-12-31).Tertiary Key (Tie-breaker 2): If both sort_start_date and sort_end_date are identical, a final tie-breaker is applied. The system can sort by the confidence_score in descending order, placing more certain events before uncertain ones. Alternatively, a stable sort algorithm can simply preserve the original input order, which may be a more desirable user-experience trait.Data TypeComparison RuleExampleAbsolute DatePrimary Key: sort_start_date.1990-01-01 comes before 1990-02-01.Fuzzy Date (same start)Secondary Key: sort_end_date.1990 (1990-01-01/1990-12-31) comes before 199X (1990-01-01/1999-12-31).Overlapping IntervalsPrimary Key: sort_start_date.[1990-01-01/1991-01-01] comes before [1990-06-01/1991-06-01].Identical Start/End DatesTertiary Key: confidence_score or Stable Sort.A photo from 1990 (confidence 1.0) appears before a photo from 1990? (confidence 0.8), or maintains its original order.This table formalizes the multi-level comparison logic, demonstrating how a custom sorting function can handle a complex, heterogeneous dataset in a predictable and user-friendly way. It directly addresses the user's requirement for sorting a timeline of various date types.5. System Integration and API ConsiderationsAPI Best Practices for Temporal DataFor consistency and interoperability, all API endpoints should exclusively accept and return dates and temporal intervals in the ISO 8601 or EDTF string format. This practice eliminates regional ambiguity and ensures that data is machine-readable and easily processed by any modern programming language or database with built-in libraries.1 The responsibility for converting user-friendly, ambiguous strings (e.g., last month) into this canonical format belongs to the front-end application or a dedicated parsing service, not the API itself. The API should serve as a consistent interface for the canonical data model, decoupling the presentation layer from the data storage and business logic.Relative Date Encoding and RefinementRelative dates, such as last week or next month, are inherently dynamic; their precise value changes based on the current date.6 They cannot be stored statically in the database as a single date point. To address this, the system treats a relative date as a transient, system-generated EDTF interval. When a user inputs last month, the application calculates the corresponding EDTF range based on the current date (e.g., 2024-03) and presents it to the user. The user can then accept this system-generated date, making it a static, fuzzy date, or provide more detail to refine it further. The original_input field in the canonical data model is crucial here, as it preserves the user's original query for potential future re-evaluation. This design fulfills the user's requirement of encoding relative dates into a fuzzy date model that can be easily adapted with additional context.6. Open-Source Specification and DocumentationThe full specification is intended to be shared as an open-source GitHub Gist. To ensure clarity and navigability, the document will be broken down into several modular markdown files, each focusing on a specific aspect of the system. The file structure will be as follows:README.md: Provides a high-level overview of the project and links to the other files.DATA_MODEL.md: Details the canonical data schema and the use of EDTF.PARSING.md: Explains the multi-layered parsing architecture, including the LLM prompts and normalization rules.SORTING.md: Describes the custom multi-level sorting algorithm and logic.AGENTS.md: The token-conscious section for AI agents.Token-Conscious AGENTS.md SectionThis section is a concise, high-level summary of the system's temporal data handling capabilities, designed to be consumed by an AI agent or a similar automation tool. Its purpose is to provide a technical contract without exceeding token limits, minimizing API costs and latency.34 The language is direct and technical, avoiding conversational or descriptive prose.AGENT.mdTemporal Data Management SpecificationCore Capability: Processes, normalizes, and sorts heterogeneous temporal data (points, ranges, fuzzy dates) for timeline visualization.Canonical Format: All temporal data is represented as an Extended Date/Time Format (EDTF) string, an ISO 8601-2 compliant format.Frontmatter Properties for Blog Posts:date: Post publication date (ISO 8601).lastmod: Post last modified date (ISO 8601).event_date: Canonical date of the event described in the post (EDTF).Format Examples:Year: YYYY (e.g., 1995)Month: YYYY-MM (e.g., 1995-07)Date: YYYY-MM-DD (e.g., 1995-07-21)Fuzzy/Approximate: YYYY~, YYYY? (e.g., 1995~, 1995-07?)Decade: YYYY-XX or 199X (e.g., 1990-XX)Range: YYYY/YYYY (e.g., 1990/1999)Open-ended range: or (e.g., [..1995], [1995..])LLM Interaction Protocol:Purpose: Disambiguate and normalize ambiguous natural language date inputs.Method: For inputs not parsed deterministically, a few-shot prompt is used to instruct the LLM to return a canonical EDTF string and a confidence score.**Example Prompt Injection:**json{"query": "I think it was around July 1995","today": "2024-04-15"}Expected LLM Output (JSON):JSON{
  "canonical_date": "1995-07~?",
  "confidence_score": 0.6
}
Constraint: LLM should not compute relative dates. It must return a relative formula for the application logic to resolve.Key Functions:parse_user_input(string user_query): Returns a canonical EDTF string and confidence score.get_timeline(sort_direction): Returns a sorted list of events based on the defined multi-level sort logic.
### **7. Conclusion**

The technical specification presented here is a robust solution for a timeline blog that must handle the complexities of human memory. The decision to adopt the Extended Date/Time Format (EDTF) as the canonical data model is the foundational element that allows the system to accurately represent absolute points, fuzzy dates, and intervals without compromising data integrity. This approach provides a clear path for normalizing ambiguous user input, which is handled by a multi-layered parsing architecture that intelligently combines a deterministic rule-based parser with a semantic LLM layer.

The LLM is strategically deployed to provide a "grace period" for user input, suggesting plausible interpretations and confidence scores rather than imposing a false sense of precision. This design improves the user experience while ensuring that the underlying data remains consistent and semantically accurate. The custom multi-level sorting algorithm, which leverages the derived `sort_start_date` and `sort_end_date` fields, ensures that the timeline is always presented in a stable, chronological, and predictable order, regardless of the data's granularity or uncertainty.

This architectural blueprint, designed for an open-source context, provides a comprehensive and scalable foundation for managing temporal data in a way that respects the nuances of human memory. Future work could include integrating fuzzy matching algorithms to correct typographical errors in user input [35, 36], implementing advanced "anytime sorting" to provide a partially-sorted timeline while the full list is being computed [37], or developing a search function that calculates relevancy based on the degree of overlap between a query's date range and the stored temporal intervals.[38] These enhancements would build upon the stable, flexible architecture outlined in this document, further improving the system's functionality and user experience.
